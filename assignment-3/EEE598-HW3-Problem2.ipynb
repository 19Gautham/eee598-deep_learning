{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d64d3def-5133-4fd2-85a3-e9e3b4c6c28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match the input size of EfficientNet\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for pre-trained models\n",
    "])\n",
    "\n",
    "# https://pytorch.org/vision/0.12/_modules/torchvision/datasets/flowers102.html\n",
    "oxford_102 = torchvision.datasets.Flowers102(root=\"./data\", download=True, transform=transform)\n",
    "\n",
    "oxford_102_val = torchvision.datasets.Flowers102(root=\"./data\", download=True, split=\"val\", transform=transform)\n",
    "# print(f\"Validation set size: {len(oxford_102_val)}\")\n",
    "oxford_102_test = torchvision.datasets.Flowers102(root=\"./data\", download=True, split=\"test\", transform=transform)\n",
    "# print(f\"Test set size: {len(oxford_102_test)}\")\n",
    "\n",
    "\n",
    "# setid = sio.loadmat(\"./data/flowers-102/setid.mat\")\n",
    "# print(setid.keys())\n",
    "\n",
    "#### Part 2\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "train_data_set = DataLoader(dataset=oxford_102, shuffle=True, batch_size=32)\n",
    "test_data_set = DataLoader(dataset=oxford_102_test, batch_size=32)\n",
    "\n",
    "efficient_net = torchvision.models.efficientnet_v2_s(progress=True)\n",
    "efficient_net.train()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=efficient_net.parameters(), lr=0.001)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a657e8-c9c6-4c21-a362-ff29aff55427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin...\n"
     ]
    }
   ],
   "source": [
    "#modified EFFNET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_v2_s\n",
    "from torchsummary import summary\n",
    "\n",
    "# Load the original EfficientNetV2-S model\n",
    "efficient_net = efficientnet_v2_s(progress=True)\n",
    "\n",
    "# Modify the architecture\n",
    "# 1. Keep the original features (convolutional layers)\n",
    "# 2. Modify the classifier to have more parameters and 102 output classes\n",
    "efficient_net.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=1024, bias=True),\n",
    "    nn.SiLU(inplace=True),\n",
    "    nn.Dropout(p=0.15, inplace=True),\n",
    "    nn.Linear(in_features=1024, out_features=512, bias=True),\n",
    "    nn.SiLU(inplace=True),\n",
    "    nn.Dropout(p=0.1, inplace=True),\n",
    "    nn.Linear(in_features=512, out_features=256, bias=True),\n",
    "    nn.SiLU(inplace=True),\n",
    "    nn.Linear(in_features=256, out_features=102, bias=True)\n",
    ")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# efficient_net.to(device)\n",
    "\n",
    "# Print model summary\n",
    "# summary(efficient_net, (3, 224, 224))\n",
    "\n",
    "print(\"Fin...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf7c37c-8a76-463c-bbec-b790889db374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:27<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.530396819114685\n",
      "Epoch 1, Training Time: 27.43110966682434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:12<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 4.398523792624474\n",
      "Epoch 2, Training Time: 12.348190069198608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [01:57<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Test Accuracy: 2.228004553585949\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train MODIFIED for 2 epochs/ 2 GPU's\n",
    "\n",
    "efficient_net = torch.nn.DataParallel(efficient_net)\n",
    "\n",
    "efficient_net.train()\n",
    "efficient_net.to(device)\n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    time_b4_train = time.time()\n",
    "    efficient_net.train()\n",
    "    for images, labels in tqdm.tqdm(train_data_set):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = efficient_net(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    time_after_train = time.time()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_data_set)}')\n",
    "    print(f'Epoch {epoch+1}, Training Time: {time_after_train-time_b4_train}')\n",
    "\n",
    "with torch.no_grad():\n",
    "  efficient_net.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for images, labels in tqdm.tqdm(test_data_set):\n",
    "    predictions = efficient_net(images.to(device))\n",
    "    _, predicted = torch.max(predictions, 1)\n",
    "    total += labels.size(0)  # Total number of labels\n",
    "    correct += (predicted == labels.to(device)).sum().item()  # Count correct predictions\n",
    "\n",
    "  print(f'Epoch {epoch+1}, Test Accuracy: {(correct/total) *100}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edbff559-1cba-4faa-a6be-f9789f672727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:14<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.626077190041542\n",
      "Epoch 1, Training Time: 14.053879261016846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:13<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 4.626089662313461\n",
      "Epoch 2, Training Time: 13.41404104232788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [01:03<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Test Accuracy: 1.4311270125223614\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MODIFIED-NET Single GPU\n",
    "\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "# train MODIFIED for 2 epochs/ 2 GPU's\n",
    "\n",
    "# efficient_net = torch.nn.DataParallel(efficient_net)\n",
    "\n",
    "efficient_net.train()\n",
    "efficient_net.to(device)\n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    time_b4_train = time.time()\n",
    "    efficient_net.train()\n",
    "    for images, labels in tqdm.tqdm(train_data_set):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = efficient_net(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    time_after_train = time.time()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_data_set)}')\n",
    "    print(f'Epoch {epoch+1}, Training Time: {time_after_train-time_b4_train}')\n",
    "\n",
    "with torch.no_grad():\n",
    "  efficient_net.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for images, labels in tqdm.tqdm(test_data_set):\n",
    "    predictions = efficient_net(images.to(device))\n",
    "    _, predicted = torch.max(predictions, 1)\n",
    "    total += labels.size(0)  # Total number of labels\n",
    "    correct += (predicted == labels.to(device)).sum().item()  # Count correct predictions\n",
    "\n",
    "  print(f'Epoch {epoch+1}, Test Accuracy: {(correct/total) *100}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a16f67d-d958-4dc2-8697-b3f5df388a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.78it/s]\n",
      "100%|██████████| 193/193 [00:45<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Accuracy: 0.35778175313059035\n",
      "Epoch 1, Loss: 5.722531795501709\n",
      "Epoch 1, Training Time: 11.513773441314697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.34it/s]\n",
      "100%|██████████| 193/193 [00:49<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Test Accuracy: 3.171247357293869\n",
      "Epoch 2, Loss: 4.5821602791547775\n",
      "Epoch 2, Training Time: 9.57150650024414\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "efficient_net = torchvision.models.efficientnet_v2_s(progress=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=efficient_net.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "# efficient_net = torch.nn.DataParallel(efficient_net)\n",
    "\n",
    "efficient_net.train()\n",
    "efficient_net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    time_b4_train = time.time()\n",
    "    efficient_net.train()\n",
    "    for images, labels in tqdm.tqdm(train_data_set):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = efficient_net(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    time_after_train = time.time()\n",
    "    with torch.no_grad():\n",
    "      efficient_net.eval()\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for images, labels in tqdm.tqdm(test_data_set):\n",
    "        predictions = efficient_net(images.to(device))\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        total += labels.size(0)  # Total number of labels\n",
    "        correct += (predicted == labels.to(device)).sum().item()  # Count correct predictions\n",
    "\n",
    "      print(f'Epoch {epoch+1}, Test Accuracy: {(correct/total) *100}')\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_data_set)}')\n",
    "    print(f'Epoch {epoch+1}, Training Time: {time_after_train-time_b4_train}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb133c55-5f66-4ad1-870a-fdc9c6ddc1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.86it/s]\n",
      "100%|██████████| 193/193 [00:53<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Accuracy: 0.3252561392096276\n",
      "Epoch 1, Loss: 5.718189403414726\n",
      "Epoch 1, Training Time: 11.17623257637024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:10<00:00,  2.96it/s]\n",
      "100%|██████████| 193/193 [00:53<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Test Accuracy: 0.9107171897869573\n",
      "Epoch 2, Loss: 4.536223724484444\n",
      "Epoch 2, Training Time: 10.823798894882202\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DATA AUGMENTATION\n",
    "\n",
    "#### 5. Transform part\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Custom Channel Inversion transformation\n",
    "class ChannelColorInversion(object):\n",
    "    def __init__(self, randomPixelChange, channels=(0, 1, 2)):  # By default, inverts all channels (R, G, B)\n",
    "        self.channels = channels\n",
    "        self.randomPixelChange = randomPixelChange\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = transforms.ToPILImage()(img)\n",
    "\n",
    "        np_img = np.array(img)\n",
    "\n",
    "        # Invert specified channels\n",
    "        for channel in self.channels:\n",
    "            # np_img[..., channel] = 255 - np_img[..., channel]\n",
    "            np_img[..., channel] = (self.randomPixelChange + np_img[..., channel])%255\n",
    "\n",
    "\n",
    "        return Image.fromarray(np_img.astype('uint8'))\n",
    "\n",
    "randomPixel = random.randint(-2, 2)\n",
    "\n",
    "# Apply the custom transformation and load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize the image\n",
    "    ChannelColorInversion(randomPixel, channels=(0, 2)),  # Invert only the Red and Blue channels\n",
    "    transforms.ToTensor(),  # Convert back to Tensor\n",
    "])\n",
    "\n",
    "# https://pytorch.org/vision/0.12/_modules/torchvision/datasets/flowers102.html\n",
    "oxford_102 = torchvision.datasets.Flowers102(root=\"./data\", download=True, transform=transform)\n",
    "\n",
    "oxford_102_val = torchvision.datasets.Flowers102(root=\"./data\", download=True, split=\"val\", transform=transform)\n",
    "# print(f\"Validation set size: {len(oxford_102_val)}\")\n",
    "oxford_102_test = torchvision.datasets.Flowers102(root=\"./data\", download=True, split=\"test\", transform=transform)\n",
    "# print(f\"Test set size: {len(oxford_102_test)}\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "train_data_set = DataLoader(dataset=oxford_102, shuffle=True, batch_size=32)\n",
    "test_data_set = DataLoader(dataset=oxford_102_test, batch_size=32)\n",
    "\n",
    "efficient_net = torchvision.models.efficientnet_v2_s(progress=True)\n",
    "efficient_net.train()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=efficient_net.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "efficient_net = torch.nn.DataParallel(efficient_net)\n",
    "\n",
    "efficient_net.train()\n",
    "efficient_net.to(device)\n",
    "\n",
    "# correctly classified image and label\n",
    "correct_classified_image = []\n",
    "correct_classified_label = []\n",
    "\n",
    "incorrect_classified_image = []\n",
    "incorrect_classified_label = []\n",
    "\n",
    "max_correct = 2\n",
    "max_incorrect = 2\n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    time_b4_train = time.time()\n",
    "    efficient_net.train()\n",
    "    for images, labels in tqdm.tqdm(train_data_set):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = efficient_net(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    time_after_train = time.time()\n",
    "    with torch.no_grad():\n",
    "      efficient_net.eval()\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for images, labels in tqdm.tqdm(test_data_set):\n",
    "        predictions = efficient_net(images.to(device))\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        total += labels.size(0)  # Total number of labels\n",
    "        correct += (predicted == labels.to(device)).sum().item()  # Count correct predictions\n",
    "\n",
    "        if max_correct > 0 and (predicted == labels.to(device)).sum().item() > 0:\n",
    "          correct_classified_image.append(images)\n",
    "          correct_classified_label.append(labels)\n",
    "          max_correct -= 1\n",
    "        if max_incorrect > 0 and (predicted == labels.to(device)).sum().item() > 0:\n",
    "          max_incorrect -= 1\n",
    "          incorrect_classified_image.append(images)\n",
    "          incorrect_classified_label.append(labels)\n",
    "\n",
    "      print(f'Epoch {epoch+1}, Test Accuracy: {(correct/total) *100}')\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_data_set)}')\n",
    "    print(f'Epoch {epoch+1}, Training Time: {time_after_train-time_b4_train}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e908eb2-dd47-486c-92ae-07daa49cd9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eee598",
   "language": "python",
   "name": "eee598"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
